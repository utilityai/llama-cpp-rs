[package]
name = "llama-cpp-2"
description = "llama.cpp bindings for Rust"
version = "0.1.44"
edition = "2021"
license = "MIT OR Apache-2.0"
repository = "https://github.com/utilityai/llama-cpp-rs"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
llama-cpp-sys-2 = { path = "../llama-cpp-sys-2", version = "0.1.44" }
thiserror = { workspace = true }
tracing = { workspace = true }

# optional deps for feature ollama, which may load models from ollama local repositories 
dirs = {version = "5", optional = true}
serde = {version = "1.0", features = ["derive"], optional = true}
serde_json = {version = "1.0", optional = true}

[dev-dependencies]
hf-hub = { workspace = true }
criterion = { workspace = true }
pprof = { workspace = true, features = ["criterion", "flamegraph"] }

anyhow = { workspace = true }
clap = { workspace = true , features = ["derive"] }


[features]
default = ["ollama"]

cublas = ["llama-cpp-sys-2/cublas"]
sampler = []
ollama = ["dirs", "serde", "serde_json"]

[lints]
workspace = true

[package.metadata.docs.rs]
features = ["sampler"]